<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lipisha Chaudhary</title>
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background-color: #fdfcf7;
      color: #222;
    }

    nav {
      background: #fff;
      border-bottom: 1px solid #eee;
      padding: 10px 40px;
      position: sticky;
      top: 0;
      z-index: 1000;
    }

    nav a {
      margin: 0 15px;
      text-decoration: none;
      color: #2b6cb0;
      font-weight: 500;
    }

    .hero {
      display: flex;
      flex-wrap: wrap;
      align-items: center;
      justify-content: center;
      padding: 60px 20px;
    }

    .hero img {
      width: 180px;
      height: 180px;
      border-radius: 50%;
      object-fit: cover;
      margin-right: 40px;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
    }

    .hero-text {
      max-width: 600px;
    }

    .hero-text h1 {
      font-size: 2.5em;
      margin: 0 0 10px;
    }

    .hero-text p {
      font-size: 1.1em;
      color: #555;
    }

    .section {
      max-width: 1000px;
      margin: 60px auto;
      padding: 0 20px;
    }

    .section h2 {
      color: #2b6cb0;
      margin-bottom: 20px;
    }

    .card-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 20px;
    }

    .card {
      background: #fff;
      border: 1px solid #eee;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
      transition: transform 0.3s ease;
      cursor: pointer;
    }

    .card:hover {
      transform: translateY(-5px);
    }

    .card h3 {
      margin-top: 0;
      color: #2b6cb0;
    }

    .modal {
      display: none;
      position: fixed;
      z-index: 999;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      overflow: auto;
      background-color: rgba(0,0,0,0.6);
    }

    .modal-content {
      background-color: #fff;
      margin: 10% auto;
      padding: 20px;
      border-radius: 8px;
      width: 90%;
      max-width: 600px;
    }

    .close {
      color: #aaa;
      float: right;
      font-size: 28px;
      font-weight: bold;
    }

    .close:hover,
    .close:focus {
      color: #000;
      text-decoration: none;
      cursor: pointer;
    }

    a {
      color: #2b6cb0;
      text-decoration: none;
    }

    .links a {
      margin-right: 20px;
    }

    footer {
      text-align: center;
      padding: 40px 0 20px;
      font-size: 0.9em;
      color: #aaa;
    }
  </style>
</head>
<body>
  <nav>
    <a href="#about">About</a>
    <a href="#interests">Research</a>
    <a href="#projects">Projects</a>
    <a href="#experience">Experience</a>
    <!-- <a href="#recognition">Recognition</a> -->
    <a href="#contact">Contact</a>
  </nav>

  <header class="hero" id="about">
    <img src="assets/potrait.jpg" alt="Lipisha Chaudhary" style="border-radius: 0; width: 300px; height: auto;">
    <div class="hero-text">
      <h1>Lipisha Chaudhary</h1>
        <p>PhD Candidate, Computer Science · University at Buffalo, SUNY</p>
        <p>I’m a fourth-year PhD student in Computer Science and Engineering at the University at Buffalo (SUNY), advised by Dr. Ifeoma Nwogu. I work as a Research Assistant in the Human Behavior Modeling Lab, where our research explores 3D human behavior modeling, pose estimation, facial anonymization, and AI for social good — with my focus centered around inclusive, multimodal learning for sign language understanding.</p>
        <p>I earned my Master’s in Computer Science from Rochester Institute of Technology (RIT), where I also worked as a Research Assistant. I was a PhD Research Intern at Dolby Labs, working on audio-visual understanding. I’ve also interned at Amazon Alexa AI for two summers as an Applied Scientist Intern, contributing to teams working on speaker understanding and ASR error detection and refinement.</p>
        <p>My research is focused on understanding how people express themselves through movement whether it’s in sign language, gestures, or other forms of nonverbal communication. I work with 3D motion data to capture the subtle, meaningful variations in hand, body, and facial movements that are often overlooked. Much of my work supports sign language understanding and generation, but I’m equally interested in applying these concepts to any context where communication goes beyond words. I believe in building systems that understand the complexity of human expression and help make technology more inclusive and responsive to everyone.</p>
        <p>I love the energy that comes from working in teams where ideas flow and people support each other. When I’m not debugging something, I’m likely in the kitchen baking, lying around doing nothing productive, and loving every minute of it.</p>
      <div class="links">
        <a href="https://www.linkedin.com/in/lipisha-chaudhary/" target="_blank">LinkedIn</a>
        <a href="https://scholar.google.com/citations?user=-7KK7vUAAAAJ&hl=en" target="_blank">Google Scholar</a>
      </div>
    </div>
  </header>

  <section class="section" id="interests">
    <h2>Research Interests</h2>
    <ul>
      <li>Interpretive and expressive human motion understanding</li>
      <li>Sign language understanding, translation, and production</li>
      <li>Multimodal representation learning for visual-language models</li>
      <li>3D mesh recovery and motion tokenization</li>
    </ul>
  </section>

  <section class="section" id="projects">
    <h2>Projects & Publications</h2>
      <div class="card-grid">
      
      <div class="card" onclick="showModal('abstract5')">
        <!-- Image Slideshow -->
        <div class="slideshow-container">
          <img class="asl3d-slide" src="publications/asl3d/token_freq1-1.png" style="display: block;" alt="F2F Image 1">
          <img class="asl3d-slide" src="publications/asl3d/translation_overview.png" style="display: none;" alt="F2F Image 2">
            <!-- Video Slide -->
          <video class="asl3d-slide" style="display: none;" autoplay muted loop playsinline>
            <source src="publications/asl3d/1_incam_rerendered.3.cmb.mp4" type="video/mp4" />
            Your browser does not support the video tag.
          </video>
          <div class="dot-container">
            <span class="dot" onclick="event.stopPropagation(); showSlide(0, 'asl3d-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(1, 'asl3d-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(2, 'asl3d-slide')"></span>
          </div>
        <h3>Large-Scale 3D Representations for ASL</h3>
        <p>NeurIPS Datasets & Benchmarks Track 2025 – Under Review</p>
      </div>

      <div class="card" onclick="showModal('abstract6')">
        <!-- Image Slideshow -->
        <div class="slideshow-container">
          <img class="f2f-slide" src="publications/f2f/main.png" style="display: block;" alt="F2F Image 1">
          <img class="f2f-slide" src="publications/f2f/F2F-3D.png" style="display: none;" alt="F2F Image 2">
          <img class="f2f-slide" src="publications/f2f/five_dancer.png" style="display: none;" alt="F2F Image 2">
          <img class="f2f-slide" src="publications/f2f/Comparison.png" style="display: none;" alt="F2F Image 2">
          <div class="dot-container">
            <span class="dot" onclick="event.stopPropagation(); showSlide(0, 'f2f-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(1, 'f2f-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(2, 'f2f-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(3, 'f2f-slide')"></span>
          </div>
        </div>
        <h3>Footwork2Framework: Capturing Fine-Grained 3D Motion Representations in Expressive Dance</h3>
        <p>NeurIPS Creative AI Track 2025 – Under Review</p>
      </div>
      <div class="card" onclick="showModal('abstract1')">
         <!-- Image Slideshow -->
        <div class="slideshow-container">
          <img class="cross-slide" src="publications/cross/asling-body-attn-plot.png" style="display: block;" alt="F2F Image 1">
          <img class="cross-slide" src="publications/cross/asling-face-attn-plot.png" style="display: none;" alt="F2F Image 2">
          <img class="cross-slide" src="publications/cross/faces.png" style="display: none;" alt="F2F Image 2">
          <img class="cross-slide" src="publications/cross/Picture6.png" style="display: none;" alt="F2F Image 2">
          <div class="dot-container">
            <span class="dot" onclick="event.stopPropagation(); showSlide(0, 'cross-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(1, 'cross-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(2, 'cross-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(3, 'cross-slide')"></span>
          </div>
        </div>
        <h3>Cross-Attention for Sign Analysis</h3>
        <p>ICPR 2024</p>
        <p><a href="https://link.springer.com/chapter/10.1007/978-3-031-78305-0_24" target="_blank">Paper Link</a></p>
      </div>
      <div class="card" onclick="showModal('abstract2')">
          <!-- Image Slideshow -->
        <div class="slideshow-container">
          <img class="sign2-slide" src="publications/signnet2/DualLearn.png" style="display: block;" alt="F2F Image 1">
          <img class="sign2-slide" src="publications/signnet2/transformer.png" style="display: none;" alt="F2F Image 2">
          <div class="dot-container">
            <span class="dot" onclick="event.stopPropagation(); showSlide(0, 'sign2-slide')"></span>
            <span class="dot" onclick="event.stopPropagation(); showSlide(1, 'sign2-slide')"></span>
          </div>
        <h3>SignNet II – Two-Way SLT</h3>
        <p>IEEE TPAMI 2022</p>
        <p><a href="https://doi.org/10.1109/TPAMI.2022.3201234" target="_blank">Paper Link</a></p>
      </div>
      <div class="card" onclick="showModal('abstract3')">
        <h3>Cross-Feature Fusion Sign Language Translation</h3>
        <p>IEEE FG 2024</p>
        <p><a href="https://ieeexplore.ieee.org/abstract/document/9667027" target="_blank">Paper Link</a></p>
      </div>
      <div class="card" onclick="showModal('abstract4')">
        <h3>Feature Scaling and Fusion on Sign Language Translation</h3>
        <p>IEEE FG 2024</p>
        <p><a href="https://www.isca-archive.org/interspeech_2021/ananthanarayana21_interspeech.pdf" target="_blank">Paper Link</a></p>
      </div>
    </div>

    <div id="abstract1" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract1')">&times;</span>
        <h3>Cross-Attention Based Influence Model for Manual and Nonmanual Sign Language Analysis</h3>
        <p>Both manual (relating to the use of hands) and non-manual markers (NMM), such as facial expressions or mouthing cues, are important for providing the complete meaning of phrases in American Sign Language (ASL). Efforts have been made in advancing sign language to spoken/written language understanding, but most of these have primarily focused on manual features only. In this work, using advanced neural machine translation methods, we examine and report on the extent to which facial expressions contribute to understanding sign language phrases. We present a sign language translation architecture consisting of two-stream encoders, with one encoder handling the face and the other handling the upper body (with hands). We propose a new parallel cross-attention decoding mechanism that is useful for quantifying the influence of each input modality on the output. The two streams from the encoder are directed simultaneously to different attention stacks in the decoder. Examining the properties of the parallel cross-attention weights allows us to analyze the importance of facial markers compared to body and hand features during a translating task.</p>
      </div>
    </div>
    <div id="abstract2" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract2')">&times;</span>
        <h3>SignNet II: A Transformer-Based Two-Way Sign Language Translation Model</h3>
        <p>SignNet II presents a bidirectional sign-to-text and text-to-sign transformer model trained on real-world sign data with aligned translations, setting a new benchmark in SLT performance.</p>
      </div>

    <div id="abstract3" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract3')">&times;</span>
        <h3>Dynamic cross-feature fusion for american sign language translation</h3>
        <p>While a significant amount of work has been done on the commonly used, tightly -constrained weather-based, German sign language (GSL) dataset, little has been done for continuous sign language translation (SLT) in more realistic settings, including American sign language (ASL) translation. Also, while CNN - based features have been consistently shown to work well on the GSL dataset, it is not clear whether such features will work as well in more realistic settings when there are more heterogeneous signers in non-uniform backgrounds. To this end, in this work, we introduce a new, realistic phrase-level ASL dataset (ASLing), and explore the role of different types of visual features (CNN embeddings, human body keypoints, and optical flow vectors) in translating it to spoken American English. We propose a novel Transformer-based, visual feature learning method for ASL translation. We demonstrate the explainability efficacy of our proposed learning methods by visualizing activation weights under various input conditions and discover that the body keypoints are consistently the most reliable set of input features. Using our model, we successfully transfer-learn from the larger GSL dataset to ASLing, resulting in significant BLEU score improvements. In summary, this work goes a long way in bringing together the AI resources required for automated ASL translation in unconstrained environments.</p>
      </div>
    </div>
    <div id="abstract4" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract4')">&times;</span>
        <h3>Effects of Feature Scaling and Fusion on Sign Language Translation</h3>
        <p>Sign language translation without transcription has only recently started to gain attention. In our work, we focus on improving the state-of-the-art translation by introducing a multifeature fusion architecture with enhanced input features. Assign language is challenging to segment, we obtain the input features by extracting overlapping scaled segments across the video and obtaining their 3D CNN representations. We exploit the attention mechanism in the fusion architecture by initially learning dependencies between different frames of the same video and later fusing them to learn the relations between different features from the same video. In addition to 3D CNN features, we also analyze pose-based features.</p>
      </div>
    <div id="abstract5" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract5')">&times;</span>
        <h3>Large-Scale 3D Representations for Continuous ASL Understanding</h3>
        <p>American Sign Language (ASL) is the primary language of Deaf and Hard-of-Hearing (D/HH) communities in the United States, Canada, parts of West Africa, and other regions worldwide (e.g., Bolivia, Jamaica, and the Philippines). Yet ASL users continue to face barriers in domains such as education, healthcare, and digital media. As a visually rich, 3D language, ASL presents challenges for computational models that rely on 2D inputs or limited 3D pose representations. To address this, we use state-of-the-art 3D pose and mesh estimation models to release large-scale, structured sign representations—including skeleton joints, SMPL-X meshes, and 3D convolutional features—extracted from over 250 hours (nearly 100,000 clips) of signing videos across two ASL datasets: OpenASL and ASLing. We also introduce OpenASL-35k, a curated 35,000-sample benchmark for ASL-to-English translation and sign production, achieving a BLEU4 score of 9.88 for ASL translation without pretraining and similarly demonstrating strong results in production. Finally, we release the complete processing pipeline—including data loaders, model checkpoints, and evaluation tools—to support reproducibility and enable further research in ASL modeling.</p>
      </div>
    <div id="abstract6" class="modal">
      <div class="modal-content">
        <span class="close" onclick="closeModal('abstract6')">&times;</span>
        <h3>Footwork2Framework: Capturing Fine-Grained 3D Motion Representations in Expressive Dance</h3>
        <p>Traditional South Asian dance forms encode complex, intricate motions, rhythms, and expressions through coordinated movements of the body and hands. Yet, they remain largely absent from computational archives, limited by scarce motion capture data and the difficulty of capturing fine-grained, synchronized motion at scale. We introduce Footwork2Framework (F2F) - the first large-scale 3D motion dataset for South Asian classical dance, specifically focusing on the Bharatanatyam dance style. It comprises over 47,000 seconds of temporally aligned full-body and detailed hand mesh sequences reconstructed from high-resolution video using state-of-the-art mesh recovery. Surpassing prior resources in scale, anatomical detail, and cultural specificity, F2F not only documents Bharatanatyam at unprecedented granularity but also releases processed 3D features to catalyze research in motion analysis, generation, and cultural preservation. As an initial exploration, we train a simple Variational Autoencoder (VAE)-based transformer, showing that the dataset’s fine-grained 3D features enable models to learn the intricate spatiotemporal structures of this performance tradition. We envision F2F as a foundation for Creative AI systems that can learn from, generate, and help safeguard the embodied knowledge of intangible cultural heritage.</p>
      </div>
    </div>
    
    </div>
  </section>

  <section class="section" id="experience">
    <h2>Industry Experience</h2>
    <div class="card-grid">
      <div class="card">
        <h3>Dolby Labs (PhD Research Intern)</h3>
        <p>Summer 2024 – Built an audio description pipeline using video LLMs and RAG; proposed new evaluation metric.</p>
      </div>
      <div class="card">
        <h3>Amazon Alexa AI (Applied Scientist Intern)</h3>
        <p>Summer 2023 – Developed error-correction system for accented speech with Alpaca-LoRA.</p>
      </div>
      <div class="card">
        <h3>Amazon Alexa AI (Applied Scientist Intern)</h3>
        <p>Summer 2022 – Designed adversarial training architecture for robust wake word detection.</p>
      </div>
    </div>
  </section>

  <!-- <section class="section" id="recognition">
    <h2>Awards & Talks</h2>
    <ul>
      <li>Invited Talk: <em>“Visual Linguistics in 3D Motion”</em>, ASL Tech Summit 2024</li>
    </ul>
  </section> -->

  <section class="section" id="contact">
    <h2>Contact</h2>
    <p><strong>Email:</strong>lipishan@buffalo.edu</p>
    <p><a href="https://www.linkedin.com/in/lipisha-chaudhary/" target="_blank">LinkedIn</a> · <a href="https://scholar.google.com/citations?user=-7KK7vUAAAAJ&hl=en" target="_blank">Google Scholar</a></p>
  </section>

  <footer>
    <p>&copy; 2025 Lipisha Chaudhary</p>
  </footer>

  <script>
    function setupAutoSlideshow(className, interval = 4000, dotSelector = null) {
      const slides = document.getElementsByClassName(className);
      const dots = dotSelector ? document.querySelectorAll(dotSelector) : [];
      let index = 0;
      let paused = false;

      function showCurrentSlide(i) {
        for (let j = 0; j < slides.length; j++) slides[j].style.display = "none";
        slides[i].style.display = "block";

        if (dots.length) {
          dots.forEach(dot => dot.classList.remove("active-dot"));
          if (dots[i]) dots[i].classList.add("active-dot");
        }
      }

      showCurrentSlide(index);

      // Automatic loop
      setInterval(() => {
        if (paused) return;
        index = (index + 1) % slides.length;
        showCurrentSlide(index);
      }, interval);

      // Dot click handling (if applicable)
      if (dots.length) {
        dots.forEach((dot, i) => {
          dot.addEventListener("click", function (e) {
            e.stopPropagation(); // prevents modal from triggering
            index = i;
            showCurrentSlide(index);
            paused = true;
            setTimeout(() => paused = false, 5000); // pause for 5s
          });
        });
      }
    }

    // Example call for your Footwork2Framework card
    document.addEventListener("DOMContentLoaded", function () {
      setupAutoSlideshow("f2f-slide", 4000, ".dot-container .dot");
    });
    </script>



  <!-- Shared Slideshow Styles -->
  <style>
    .slideshow-container {
    max-width: 100%;
    position: relative;
    margin-bottom: 10px;
    }
    .asl3d-slide, .f2f-slide, .cross-slide, .sign2-slide {
    width: 100%;
    max-height: 250px;
    object-fit: cover;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    .dot-container {
    text-align: center;
    margin-top: 5px;
    }
    .dot {
    height: 10px;
    width: 10px;
    margin: 0 2px;
    background-color: #bbb;
    border-radius: 50%;
    display: inline-block;
    cursor: pointer;
    }
    .dot.active-dot {
      background-color: #333;
    }
    .dot:hover {
    background-color: #717171;
    }
  </style>

  <script>
    function showModal(id) {
      document.getElementById(id).style.display = "block";
    }
    function closeModal(id) {
      document.getElementById(id).style.display = "none";
    }
  </script>
</body>
</html>
